# -*- coding: utf-8 -*-
"""NMF_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18RVg7k-e7zAthW4r6t6hOr2L8vIe5xQz
"""
import keras
from keras.models import Sequential, Model
from keras.layers import Dense , merge
from keras.layers.merge import dot, Concatenate
from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop
from keras.layers import Dropout, Flatten,Activation,Input,Embedding
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd
import numpy as np


import tensorflow as tf

import math

def MF_layer(n_movies,n_users,n_latent_factors):
  user_input=Input(shape=(1,),name='user_input',dtype='int64')
  movie_input=Input(shape=(1,),name='movie_input',dtype='int64')

  user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)
  movie_embedding=Embedding(n_movies,n_latent_factors,name='movie_embedding')(movie_input)

  user_vec = Flatten(name='FlattenUsers')(user_embedding)
  movie_vec = Flatten(name='FlattenMovies')(movie_embedding)

  sim = dot([user_vec,movie_vec],name='Simalarity-Dot-Product',axes=1)

  prediction = Dense(1, activation='linear', name = 'prediction')(sim)
  model = keras.models.Model([user_input, movie_input],prediction)

  return model

def MLP_layer(n_movies,n_users,n_latent_factors, layers = []):
  num_layer = len(layers)
  user_input=Input(shape=(1,),name='user_input',dtype='int64')
  movie_input=Input(shape=(1,),name='movie_input',dtype='int64')

  user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)
  movie_embedding=Embedding(n_movies,n_latent_factors,name='movie_embedding')(movie_input)

  user_vec = Flatten(name='FlattenUsers')(user_embedding)
  movie_vec= Flatten(name='FlattenMovies')(movie_embedding)

  sim = keras.layers.Concatenate(name="MLP_input_layer")([user_vec, movie_vec])

  for idx in range(1, num_layer):
    layer = Dense(layers[idx], activation='relu', name = 'layer%d' %idx)
    sim = layer(sim)
  prediction = Dense(1, activation='relu', name = 'prediction')(sim)

  model = keras.models.Model([user_input, movie_input],prediction)

  return model

def NMF(n_movies, n_users, NMF_layer_units, MLP_layers,lr,n_latent_factors = 10):
  num_layer = len(MLP_layers)
  user_input=Input(shape=(1,),name='user_input',dtype='int64')
  movie_input=Input(shape=(1,),name='movie_input',dtype='int64')

  MF_user_embedding=Embedding(n_users,n_latent_factors,name='MF_user_embedding')(user_input)
  MF_movie_embedding=Embedding(n_movies,n_latent_factors,name='MF_movie_embedding')(movie_input)

  MLP_user_embedding=Embedding(n_users,n_latent_factors,name='MLP_user_embedding')(user_input)
  MLP_movie_embedding=Embedding(n_movies,n_latent_factors,name='MLP_movie_embedding')(movie_input)

  MF_user_vec = Flatten(name='MF_FlattenUsers')(MF_user_embedding)
  MF_movie_vec= Flatten(name='MF_FlattenMovies')(MF_movie_embedding)
  MF_vector=dot([MF_user_vec,MF_movie_vec],name='MF_dot_product',axes=1)

  MLP_user_vec = Flatten(name='MLP_FlattenUsers')(MLP_user_embedding)
  MLP_movie_vec= Flatten(name='MLP_FlattenMovies')(MLP_movie_embedding)
  MLP_vector = keras.layers.Concatenate(name="MLP_input_vector")([MLP_user_vec, MLP_movie_vec])

  for idx in range(0, num_layer):
    layer = Dense(MLP_layers[idx], activation='relu', name = 'MLP_layer%d' %idx)
    MLP_vector = layer(MLP_vector)

  #NMF layer
  predict_vector = keras.layers.Concatenate(name="NMF_input_vector")([MF_vector, MLP_vector])
  NMF_layer = Dense(NMF_layer_units, activation='relu', name = 'NMF_hidden_layer')(predict_vector)
  prediction = Dense(1, activation='relu', name = 'NMF_output_layer')(NMF_layer)

  model = keras.models.Model([user_input, movie_input],prediction)
  
  model.compile(optimizer=Adam(lr=lr),loss='mean_squared_error')
  

  return model
  
def NMF_Gsearch(train,val,n_batch_sizes,lr,n_epochs):
    es = EarlyStopping(monitor = "val_loss", patience = 3, mode = "min", verbose = 0)
    column_names = n_batch_sizes #batch sizes 
    df = pd.DataFrame(columns = column_names)
    row = []
    
    for i in lr:
        for j in n_batch_sizes:
            model = NMF(n_movies=len(train['movieID'].unique()),n_users=len(train['userID'].unique()), n_latent_factors =                         10, NMF_layer_units = 30, MLP_layers = [30,20,10],lr=i)
            History = model.fit([train.userID,train.movieID],train.rating, batch_size=j,
                              epochs = n_epochs, validation_data = ([val.userID,val.movieID],val.rating),callbacks=[es],
                              verbose = 0)
            result = History.history['val_loss']
            row.append(math.sqrt(min(result)))
        a_series = pd.Series(row, index = df.columns)
        df = df.append(a_series, ignore_index=True)
        row = []
        
    df.insert(0, "Learning_rate", lr, True)
    df.set_index('Learning_rate',inplace = True)
    
    return df
    
    
def NMF_find_best_layers(train,val,n_batch_sizes,lr,nr_epochs,layers):
    
    es = EarlyStopping(monitor = "val_loss", patience = 3, mode = "min", verbose = 0)
    rmse = []
    
    for layer in layers:
        model = NMF(n_movies=len(train['movieID'].unique()),n_users=len(train['userID'].unique()), n_latent_factors =                         10, NMF_layer_units = layer[0], MLP_layers = layer ,lr=lr)
        History = model.fit([train.userID,train.movieID],train.rating, batch_size=n_batch_sizes,
                              epochs = nr_epochs, validation_data = ([val.userID,val.movieID],val.rating),callbacks=[es],
                              verbose = 0)
        result = History.history['val_loss']
        rmse.append(math.sqrt(min(result)))
        
    min_rmse = min(rmse)
    index = np.argmin(rmse)
    best_layers = layers[index]
    
    return (min_rmse,best_layers)
    
def NMF_factors(train,val,n_batch_sizes,lr,nr_epochs,layers,n_factors):
    
    es = EarlyStopping(monitor = "val_loss", patience = 3, mode = "min", verbose = 1)
    rmse = []
    num_factors = []
    
    for i in n_factors:
        print("fit n_latent factor ----> ",i)
        model = NMF(n_movies=len(train['movieID'].unique()),n_users=len(train['userID'].unique()), n_latent_factors =                         i, NMF_layer_units = layers[0], MLP_layers = layers ,lr=lr)
        History = model.fit([train.userID,train.movieID],train.rating, batch_size=n_batch_sizes,
        epochs = nr_epochs, validation_data = ([val.userID,val.movieID],val.rating),callbacks=[es],
        verbose = 0)
        result = History.history['val_loss']
        rmse.append(math.sqrt(min(result)))
        num_factors.append(i)
        
    
    return (rmse,num_factors)
    
def NMF_layers(train,val,n_batch_sizes,lr,nr_epochs,layers):
    
    n_layers = []
    for layer in layers:
        num = len(layer)
        n_layers.append(num)
    
    es = EarlyStopping(monitor = "val_loss", patience = 3, mode = "min", verbose = 0)
    rmse = []
    
    for layer in layers:
        model = NMF(n_movies=len(train['movieID'].unique()),n_users=len(train['userID'].unique()), n_latent_factors = 10,
        NMF_layer_units = layer[0], MLP_layers = layer ,lr=lr)
        History = model.fit([train.userID,train.movieID],train.rating, batch_size=n_batch_sizes,
                              epochs = nr_epochs, validation_data = ([val.userID,val.movieID],val.rating),callbacks=[es],
                              verbose = 1)
        result = History.history['val_loss']
        rmse.append(math.sqrt(min(result)))
        
    
    return (rmse,n_layers)